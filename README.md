<div align="center">
  <h1>Retail Analytics Strategy Engine</h1>
  <p>
    <b>
      <a href="#-english-version">ğŸ‡ºğŸ‡¸ English Version</a> | 
      <a href="#-ä¸­æ–‡ç‰ˆ">ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ</a>
    </b>
  </p>
</div>

---

<div id="-english-version"></div>

## ğŸ‡ºğŸ‡¸ English Version

**Retail Analytics Strategy Engine** is a comprehensive data analysis solution designed to uncover customer value, predict churn risk, and improve cross-selling rates through machine learning algorithms. The project includes three core analysis modules: Customer Segmentation, Propensity Scoring, and Recommendation System.

### ğŸ“‹ Key Features

#### 1. Customer Segmentation
- **File**: `scripts/customer_segmentation.py`
- **Algorithm**: K-Means Clustering + PCA (Principal Component Analysis)
- **Functionality**:
  - Based on RFM (Recency, Frequency, Monetary) model and channel preferences.
  - Automatically segments customers into **5 strategic groups** (n_clusters=5).
  - Generates profile reports to identify high-value customers and potential churners.

#### 2. Propensity Scoring & Churn Modeling
- **File**: `scripts/propensity_score.py`
- **Algorithm**: XGBoost Classifier + SHAP (Explainability) + SMOTE (Oversampling)
- **Functionality**:
  - **Churn Prediction**: Identifies customers at risk of leaving.
  - **Loyalty Prediction**: Discovers high-potential loyal customers.
  - Provides SHAP value analysis to explain key drivers (e.g., recency, total spend).

#### 3. Recommendation Engine
- **File**: `scripts/recommendation.py`
- **Algorithm**: Apriori (Market Basket Analysis)
- **Functionality**:
  - Mines strong Association Rules between products.
  - Generates "bundle suggestions" based on Support, Confidence, and Lift.
  - Ideal for designing Cross-sell and Bundling strategies (e.g., QLab Tea case).

### ğŸ› ï¸ Tech Stack
- **Data Processing**: `pandas`, `numpy`
- **Machine Learning**: `scikit-learn`, `xgboost`, `mlxtend`, `imbalanced-learn`
- **Visualization**: `matplotlib`, `seaborn`, `shap`

<br>
<div align="right">
  <a href="#-ä¸­æ–‡ç‰ˆ">Go to Chinese Version ğŸ‡¨ğŸ‡³</a>
</div>
<br>

---

<div id="-ä¸­æ–‡ç‰ˆ"></div>

## ğŸ‡¨ğŸ‡³ ä¸­æ–‡ç‰ˆ

**é›¶å”®åˆ†æç­–ç•¥å¼•æ“ (Retail Analytics Strategy Engine)** è¿™æ˜¯ä¸€ä¸ªç»¼åˆæ€§çš„é›¶å”®æ•°æ®åˆ†æè§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨é€šè¿‡æœºå™¨å­¦ä¹ ç®—æ³•æŒ–æ˜å®¢æˆ·ä»·å€¼ã€é¢„æµ‹æµå¤±é£é™©å¹¶æå‡äº¤å‰é”€å”®ç‡ã€‚é¡¹ç›®åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒåˆ†ææ¨¡å—ï¼šå®¢æˆ·åˆ†ç¾¤ã€å€¾å‘æ€§è¯„åˆ†å’Œå•†å“æ¨èç³»ç»Ÿã€‚

### ğŸ“‹ æ ¸å¿ƒåŠŸèƒ½ (Key Features)

#### 1. å®¢æˆ·åˆ†ç¾¤ (Customer Segmentation)
- **æ–‡ä»¶**: `scripts/customer_segmentation.py`
- **ç®—æ³•**: K-Means Clustering + PCA (ä¸»æˆåˆ†åˆ†æ)
- **åŠŸèƒ½**:
  - åŸºäº RFM (Recency, Frequency, Monetary) æ¨¡å‹åŠæ¸ é“åå¥½ç‰¹å¾ã€‚
  - è‡ªåŠ¨å°†å®¢æˆ·åˆ’åˆ†ä¸º **5 ä¸ªæˆ˜ç•¥ç¾¤ä½“** (n_clusters=5)ã€‚
  - ç”Ÿæˆåˆ†ç¾¤ç”»åƒæŠ¥å‘Šï¼Œè¯†åˆ«é«˜ä»·å€¼å®¢æˆ·ä¸æ½œåœ¨æµå¤±å®¢æˆ·ã€‚

#### 2. å€¾å‘æ€§è¯„åˆ†ä¸æµå¤±é¢„æµ‹ (Propensity & Churn Modeling)
- **æ–‡ä»¶**: `scripts/propensity_score.py`
- **ç®—æ³•**: XGBoost Classifier + SHAP (å¯è§£é‡Šæ€§åˆ†æ) + SMOTE (è¿‡é‡‡æ ·)
- **åŠŸèƒ½**:
  - **æµå¤±é¢„æµ‹ (Churn Risk)**: è¯†åˆ«å³å°†æµå¤±çš„å®¢æˆ·ã€‚
  - **å¿ è¯šåº¦é¢„æµ‹ (Loyalty)**: æŒ–æ˜é«˜æ½œåŠ›çš„å¿ è¯šå®¢æˆ·ã€‚
  - æä¾› SHAP å€¼åˆ†æï¼Œè§£é‡Šå½±å“æ¨¡å‹å†³ç­–çš„å…³é”®å› å­ï¼ˆå¦‚ï¼šæœ€è¿‘ä¸€æ¬¡è´­ä¹°æ—¶é—´ã€æ¶ˆè´¹é‡‘é¢ç­‰ï¼‰ã€‚

#### 3. æ™ºèƒ½æ¨èå¼•æ“ (Recommendation Engine)
- **æ–‡ä»¶**: `scripts/recommendation.py`
- **ç®—æ³•**: Apriori (è´­ç‰©ç¯®åˆ†æ / Market Basket Analysis)
- **åŠŸèƒ½**:
  - æŒ–æ˜å•†å“é—´çš„å¼ºå…³è”è§„åˆ™ (Association Rules)ã€‚
  - åŸºäºæ”¯æŒåº¦ (Support)ã€ç½®ä¿¡åº¦ (Confidence) å’Œæå‡åº¦ (Lift) ç”Ÿæˆâ€œè´­ä¹°ç»„åˆå»ºè®®â€ã€‚
  - é€‚ç”¨äºè®¾è®¡äº¤å‰é”€å”® (Cross-sell) å’Œæ†ç»‘é”€å”® (Bundling) ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼šQLab Tea æ¡ˆä¾‹ï¼‰ã€‚

### ğŸ› ï¸ æŠ€æœ¯æ ˆ (Tech Stack)
- **æ•°æ®å¤„ç†**: `pandas`, `numpy`
- **æœºå™¨å­¦ä¹ **: `scikit-learn`, `xgboost`, `mlxtend`, `imbalanced-learn`
- **å¯è§†åŒ–**: `matplotlib`, `seaborn`, `shap`

<br>
<div align="right">
  <a href="#-english-version">å›åˆ°è‹±æ–‡ç‰ˆæœ¬ ğŸ‡ºğŸ‡¸</a>
</div>

## Executive Summary
A leading tea retailer in Southeast Asia (Singapore, Malaysia, Hong Kong) faces increasing customer acquisition costs and stagnant Average Order Value (AOV). 

This project implements a **CRISP-DM** based analytical framework to solve two critical business problems:
1.  **Who is leaving?** (Churn Prediction & Prevention)
2.  **What else will they buy?** (Basket Optimization & Cross-selling)

**Compliance Note:** To adhere to Data Privacy regulations (GDPR/PDPA) and Non-Disclosure Agreements (NDA), the original proprietary dataset has been replaced. This repository utilizes a custom **Synthetic Data Generator (`data_generator.py`)** that statistically replicates the client's data distribution, seasonal patterns, and churn behaviors without exposing sensitive information.

---

## Project Architecture

The solution is divided into three strategic modules:

| Module | Business Goal | Technical Approach |
| :--- | :--- | :--- |
| **1. Data Simulation** | Ensure compliance & reproducibility. | `Faker`, `NumPy` for probabilistic generation of Demographics & Transactions. |
| **2. Customer Segmentation** | Identify high-value personas. | **K-Means Clustering** + RFM Analysis (Recency, Frequency, Monetary). |
| **3. Churn Prediction** | Proactive retention. | **XGBoost Classifier** + **SMOTE** (Imbalance handling) + **SHAP** (Explainability). |
| **4. Market Basket Analysis** | Increase wallet share (AOV). | **Apriori Algorithm** for Association Rule Mining. |

---

## Key Insights & Business Translation

### 1. The "Why" Behind Churn (Explainable AI)
Using SHAP (SHapley Additive exPlanations), we deconstructed the black-box model to understand driver behaviors.

![SHAP Summary](results/shap_feature_importance.png)
*(Note: Visual based on mock data replication)*

* **Insight:** `Recency` (days since last purchase) is the #1 predictor of churn. The risk creates a "hockey stick" curve after **60 days** of inactivity.
* **Action:** Implement an automated "We Miss You" email trigger specifically at **Day 45** (preventative) and **Day 60** (reactive) with a time-bound discount.

### 2. Strategic Product Bundling
By analyzing co-occurrence patterns in transaction history:

![Correlation Heatmap](results/product_cross_sell_matrix.png)

* **Insight:** Strong correlation found between **"Premium Pu-erh"** and **"Tea Accessories"**.
* **Action:** Create a "Gong Fu Cha Starter Set" bundle. When a user adds Pu-erh to the cart, recommend the accessory kit for 15% off.

### 3. Customer Personas (Clustering)
* **"Loyal Connoisseurs":** High Frequency, High Monetary, Low Recency. -> *Strategy: Exclusive tastings (Gold Tier).*
* **"Dormant Big Spenders":** Low Frequency, High Monetary, High Recency. -> *Strategy: High-value win-back campaigns.*

---

## Repository Structure

```bash
Retail-Analytics-Strategy-Engine/
â”œâ”€â”€ data/                     # Folder for generated CSVs (excluded from git)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ data_generator.py     # Simulates realistic retail data (Privacy Safe)
â”‚   â”œâ”€â”€ segmentation.py       # RFM Analysis & K-Means Clustering
â”‚   â”œâ”€â”€ churn_prediction.py   # XGBoost Training & SHAP Interpretation
â”‚   â””â”€â”€ market_basket.py      # Association Rules (Apriori) engine
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ shap_feature_importance.png
â”‚   â”œâ”€â”€ product_cross_sell_matrix.png
â”‚   â””â”€â”€ k_selection_report.csv
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Getting Started

Follow these instructions to set up the environment and replicate the analysis.

### 1. Prerequisites
Ensure you have Python 3.8+ installed. The project relies on the following key libraries:
* `pandas` & `numpy` (Data Manipulation)
* `scikit-learn` & `xgboost` (Modeling)
* `shap` (Model Interpretability)
* `mlxtend` (Association Rules)
* `faker` (Data Simulation)

### 2. Installation
Clone the repository and install dependencies:

```bash
git clone [https://github.com/YourUsername/Retail-Analytics-Strategy-Engine.git](https://github.com/YourUsername/Retail-Analytics-Strategy-Engine.git)
cd Retail-Analytics-Strategy-Engine

# It is recommended to create a virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

pip install -r requirements.txt
```

### 3. Usage Guide

**Step 1: Generate Synthetic Data**
First, run the generator to create the mock datasets. This ensures you have the necessary CSV files (`mock_customer_profile.csv` and `mock_transaction_history.csv`) in your local environment.

```bash
python scripts/data_generator.py
```
Output: Files generated in the project root or specified data folder.

**Step 2: Run Customer Segmentation**
Identify customer clusters based on purchasing behavior.

```bash
python scripts/segmentation.py
```
Output: Generates `customer_with_segments.csv` and K-Selection reports.

**Step 3: Train Churn Model & View Explainability**
Train the XGBoost model and generate SHAP plots to understand churn drivers.

```bash
python scripts/churn_prediction.py
```
Output: Displays/Saves SHAP summary plots and prints model accuracy metrics.

**Step 4: Generate Cross-sell Rules**
Run the Market Basket Analysis to find product associations.

```bash
python scripts/market_basket.py
```
Output: Prints top association rules (e.g., "If Buy A -> Then Buy B") and saves the correlation heatmap.

---

## Future Roadmap (Scalability)

To transition this project from a prototype to a production-ready system, the following improvements are proposed based on the **Data Science Lifecycle**:

### 1. Model Deployment (MLOps)
* **Current State:** Scripts run locally via CLI.
* **Future State:** Wrap the XGBoost model in a **FastAPI** microservice. Build a **Streamlit** dashboard for the Marketing Team to input a Customer ID and get real-time "Churn Risk Score" and "Next Best Action".

### 2. Advanced Metrics (CLV)
* **Current State:** Focus on Churn Probability.
* **Future State:** Implement **Customer Lifetime Value (CLV)** prediction using the *BG/NBD model (Lifetimes library)*. This will allow the business to prioritize retention budget on high-value customers rather than just high-risk ones.

### 3. Automated Pipeline
* **Current State:** Manual execution.
* **Future State:** Use **Apache Airflow** to schedule weekly retraining of the model as new transaction data arrives, ensuring the model adapts to changing market trends (Concept Drift).

---

## Contribution

Contributions are welcome! This project follows the standard "Fork & Pull" open-source model.

1.  **Fork** the Project
2.  Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3.  Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4.  Push to the Branch (`git push origin feature/AmazingFeature`)
5.  Open a **Pull Request**

---

## License

* **License:** Distributed under the MIT License. See `LICENSE` for more information.

---

*If you found this project useful for understanding Retail Analytics, please give it a â­ï¸!*
